package Experiments


import java.util.Calendar
import SparkER.DataStructures.{WeightedEdge}
import SparkER.EntityClustering.{EntityClusterUtils, MarkovClustering}
import SparkER.SimJoins.Commons.CommonFunctions
import SparkER.SimJoins.SimJoins.PPJoin
import SparkER.Wrappers.CSVWrapper
import org.apache.log4j.{FileAppender, Level, LogManager, SimpleLayout}
import org.apache.spark.{SparkConf, SparkContext}
/**
  * Tests the clustering methods for Febrl datasets.
  */
object EntityClusteringExperimentTests {

  def main(args: Array[String]): Unit = {

    /*
    * First: we need to obtain the scored pairs of entity profiles, we can use any of the proposed methods.
    * Here we employ PPJOIN.
    * */

    /* Dataset to test */
    val dataset = "dataset_10k"

    /* Base path where the dataset is located */
    val basePath = "datasets/dirty/febrl/" + dataset + "/"
    val fileName = "dataset.csv"

    /* Profiles to join */
    //
    val filePath = basePath + fileName
    /* Groundtruth */
    val gtPath = basePath + dataset + "groundtruth.csv"

    val startTime = Calendar.getInstance().getTimeInMillis
    /** Log file */
    val logPath = "log.txt"

    /** Spark configuration */
    val conf = new SparkConf()
      .setAppName("Main")
      .setMaster("local[*]")
      .set("spark.executor.memory", "4g")
      .set("spark.driver.memory", "4g")

    val sc = new SparkContext(conf)

    val log = LogManager.getRootLogger
    log.setLevel(Level.INFO)
    val layout = new SimpleLayout()
    val appender = new FileAppender(layout, logPath, false)
    log.addAppender(appender)

    /* Loads the profiles and extracts all fields as text documents */
    val profiles = CSVWrapper.loadProfiles2(filePath, realIDField = "rec_id", header = true)
    val maxProfileId = profiles.map(_.id).max()


    val docs = CommonFunctions.extractAllFields(profiles)
    docs.cache()
    val nd = docs.count()
    log.info("[PPJoin] Number of docs " + nd)

    /** Performs the join, keeping all the pairs with a similarity >= threshold */
    val t1 = Calendar.getInstance().getTimeInMillis
    val threshold = 0.5 // 3
    val matches = PPJoin.getMatches(docs, threshold)

    val t2 = Calendar.getInstance().getTimeInMillis

    log.info("[PPJoin] Join+verification time (s) " + (t2 - t1) / 1000.0)

    /** Now we have the candidate pairs, we can create the edges for the clustering methods */
    val edges = matches.map(x => WeightedEdge(x._1, x._2, x._3))

    /** Pick one algorithm (Same as EntityClusteringTests) */
    val cc1 = MarkovClustering.getClusters(profiles = profiles, edges, maxProfileId, edgesThreshold = 0.5, separatorID = -1)

    val endTime = Calendar.getInstance().getTimeInMillis
    /** Loads the groundtruth */
    val groundtruth = CSVWrapper.loadGroundtruth(gtPath)

    /** Converts the ids in the groundtruth to the autogenerated ones */
    val realIdIds = sc.broadcast(profiles.map { p =>
      (p.originalID, p.id)
    }.collectAsMap())

    var newGT: Set[(Int, Int)] = null
    newGT = groundtruth.map { g =>
      val first = realIdIds.value.get(g.firstEntityID)
      val second = realIdIds.value.get(g.secondEntityID)
      if (first.isDefined && second.isDefined) {
        val f = first.get
        val s = second.get
        if (f < s) (f, s) else (s, f)
      }
      else {
        (-1, -1)
      }
    }.filter(_._1 >= 0).collect().toSet

    println("maxProfileId " + maxProfileId)
    println("Number of clusters " + cc1.count())
    println("Number of profiles " + cc1.flatMap(_._2).count())
    println("Recall, precision: " + EntityClusterUtils.calcPcPqCluster(cc1, sc.broadcast(newGT)))
    println("Time taken (s) " + (endTime - startTime) / 1000.0)
    sc.stop()
  }


}
